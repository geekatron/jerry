# EN-001: Research Effective Jerry Prompt Patterns

> **Type:** enabler
> **Status:** completed
> **Priority:** high
> **Impact:** high
> **Enabler Type:** exploration
> **Created:** 2026-02-18T00:00:00Z
> **Due:** —
> **Completed:** 2026-02-18T18:00:00Z
> **Parent:** PROJ-006-jerry-prompt
> **Owner:** Tyler
> **Effort:** 8

---

## Document Sections

| Section | Purpose |
|---------|---------|
| [Summary](#summary) | What this spike researches |
| [Problem Statement](#problem-statement) | Why this research is needed |
| [Business Value](#business-value) | How findings benefit Jerry users |
| [Technical Approach](#technical-approach) | Research methodology |
| [Children (Tasks)](#children-tasks) | Task breakdown |
| [Progress Summary](#progress-summary) | Current status |
| [Acceptance Criteria](#acceptance-criteria) | Definition of done |
| [Risks and Mitigations](#risks-and-mitigations) | Known risks |
| [History](#history) | Change log |

---

## Summary

Research what prompts are most effective when working with Jerry — including prompt structure, skill invocation patterns, agent composition, quality thresholds, and outcome consistency. The goal is to produce actionable prompt guidance for Jerry users.

**Technical Scope:**
- Prompt structure patterns (directive vs. conversational, specificity levels)
- Skill invocation patterns (single skill vs. multi-skill composition)
- Agent orchestration patterns (parallel vs. sequential, critic integration)
- Quality outcome correlation (what prompt traits produce higher quality results)
- Anti-patterns that degrade Jerry's performance

---

## Enabler Type Classification

**This Enabler Type:** EXPLORATION

This is a research spike — no code deliverables, purely knowledge artifacts.

---

## Problem Statement

Jerry users currently rely on intuition when composing prompts. There is no documented guidance on what prompt structures, skill invocations, or agent compositions produce the best outcomes. The example prompt provided (Salesforce/PCS research) demonstrates a high-quality multi-skill prompt, but the *why* behind its effectiveness has not been analyzed.

Without this research:
- New Jerry users have a steep learning curve
- Prompt quality varies widely across users
- Jerry's full capabilities (skills, agents, orchestration) are underutilized

---

## Business Value

Producing a Jerry prompt best-practices guide will:
- Reduce onboarding time for new Jerry users
- Increase consistency of Jerry output quality
- Maximize utilization of Jerry's skill/agent ecosystem
- Establish a reference template library for common research patterns

### Features Unlocked

- Prompt template library for Jerry users
- Quality rubric for evaluating Jerry prompts
- Training material for Jerry adoption

---

## Technical Approach

Use Jerry's own skill stack to research itself:

1. **`/orchestration`** — Plan multi-phase research workflow with adversarial critics (quality >= 0.92)
2. **`/problem-solving`** — Deploy all 8 agents (researcher, analyst, synthesizer, architect, reviewer, investigator, validator, reporter) across research phases
3. **Analysis of existing prompts** — Deconstruct the example prompt and Jerry's own CLAUDE.md/skill invocation patterns
4. **External research** — Survey prompt engineering best practices from Anthropic documentation, academic sources, and industry guides
5. **Synthesis** — Produce actionable guidance document

---

## Children (Tasks)

### Task Inventory

| ID | Title | Status | Effort | Owner |
|----|-------|--------|--------|-------|
| — | Tasks to be generated by orchestration planner | — | — | — |

---

## Progress Summary

### Status Overview

```
+------------------------------------------------------------------+
|                   ENABLER PROGRESS TRACKER                        |
+------------------------------------------------------------------+
| Tasks:     [.......................] 0% (0/? completed)           |
| Effort:    [.......................] 0% (0/8 points completed)    |
+------------------------------------------------------------------+
| Overall:   [.......................] 0%                            |
+------------------------------------------------------------------+
```

---

## Acceptance Criteria

### Definition of Done

- [ ] Prompt anatomy documented (what structural elements matter)
- [ ] Skill invocation patterns cataloged (single, multi, orchestrated)
- [ ] Agent composition patterns analyzed (which combinations work best)
- [ ] Quality correlation data collected (prompt traits vs. output quality)
- [ ] Anti-patterns identified and documented
- [ ] Best-practices guide produced in `synthesis/`
- [ ] At least 3 prompt templates created for common Jerry tasks

---

## Risks and Mitigations

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| Subjectivity in "effectiveness" | High | Medium | Define quality rubric upfront with measurable criteria |
| Limited prompt corpus | Medium | Medium | Analyze Jerry's own skill files + user example as primary data |
| Self-referential bias (Jerry researching Jerry) | Medium | Low | Include external prompt engineering research as counterbalance |

---

## History

| Date | Author | Status | Notes |
|------|--------|--------|-------|
| 2026-02-18 | Claude | pending | Spike created |
| 2026-02-18 | Claude | in_progress | Research initiated |
