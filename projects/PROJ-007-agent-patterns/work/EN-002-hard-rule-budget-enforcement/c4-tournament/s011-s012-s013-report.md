# EN-002: S-011 / S-012 / S-013 Combined Adversarial Report

> **Strategies:** S-011 Chain-of-Verification, S-012 FMEA, S-013 Inversion
> **Target:** EN-002 Implementation (HARD Rule Budget Enforcement)
> **Executor:** adv-executor
> **Date:** 2026-02-21
> **Tournament:** C4 (>= 0.95 threshold)

---

## Document Sections

| Section | Purpose |
|---------|---------|
| [S-011 Chain-of-Verification](#s-011-chain-of-verification) | Fact-checking each implementation claim |
| [S-012 FMEA](#s-012-fmea) | Failure mode analysis per component |
| [S-013 Inversion](#s-013-inversion) | Inverting key claims to surface hidden assumptions |
| [Findings](#findings) | Classified findings: CRITICAL / MAJOR / MINOR / OBSERVATION |

---

## S-011 Chain-of-Verification

Each claim from the implementation summary is independently verified against
the actual codebase, file contents, and computed outputs. Verification method
is stated for each claim.

---

### Claim 1: "L2 coverage increased from 32% to 84%"

**Method:** Count H-rules covered by L2 markers before and after.

**Before (single file, quality-enforcement.md, 8 markers):**

The 8 markers in quality-enforcement.md reference these H-rules:
- rank=1: H-01, H-02, H-03 (constitutional)
- rank=2: H-13, H-14, H-31 (quality gate)
- rank=3: H-05, H-06 (UV — referenced as "H-05/H-06")
- rank=4: S-014 only (strategy, not an H-rule)
- rank=5: H-15
- rank=6: criticality levels (no direct H-rule citation)
- rank=8: H-19

Pre-EN-002 covered rules: H-01, H-02, H-03, H-05, H-06, H-13, H-14, H-15, H-19, H-31 = **10 rules**

Pre-EN-002 total: 31 HARD rules

Coverage: 10/31 = **32.3%** -- matches the claim.

**After (9 files, 16 markers total):**

All 16 markers and their H-rule coverage (verified by direct file inspection):

| File | Markers | H-rules Covered |
|------|---------|-----------------|
| architecture-standards.md | 1 | H-07, H-10 |
| coding-standards.md | 1 | H-11, H-12 |
| mandatory-skill-usage.md | 1 | H-22 |
| markdown-navigation-standards.md | 1 | H-23, H-24 |
| mcp-tool-standards.md | 1 | (no direct H-rule ID cited) |
| python-environment.md | 1 | H-05, H-06 |
| quality-enforcement.md | 8 | H-01..H-03, H-05..H-06, H-13..H-15, H-19, H-31 |
| skill-standards.md | 1 | H-23, H-25, H-26 |
| testing-standards.md | 1 | H-20, H-21 |

Post-EN-002 covered unique H-rules: H-01, H-02, H-03, H-05, H-06, H-07, H-10, H-11, H-12, H-13, H-14, H-15, H-19, H-20, H-21, H-22, H-23, H-24, H-25, H-26, H-31 = **21 rules**

Post-EN-002 total: 25 HARD rules

Coverage: 21/25 = **84.0%** -- matches the claim exactly.

**VERDICT: VERIFIED.** The 32% and 84% figures are arithmetically correct.

**Caveat:** The mcp-tool-standards.md marker (rank=9) covers MCP governance but
cites no H-rule ID explicitly. Whether this constitutes "H-rule coverage" is
ambiguous. The Two-Tier Model in quality-enforcement.md does not list MCP
coverage as covering any specific H-rule. This caveat does not change the
coverage percentage because MCP was not counted as an H-rule in either baseline.

---

### Claim 2: "HARD rule count reduced from 31 to 25"

**Method:** Count unique H-rule IDs in the HARD Rule Index section using the
ceiling script logic (`count_hard_rules`), and cross-reference with git history
via the implementation summary's pre/post comparison.

**Actual count (verified by running scripts/check_hard_rule_ceiling.py):**

```
PASS: HARD rule count = 25, ceiling = 25, headroom = 0 slots
```

The 25 rules are: H-01, H-02, H-03, H-04, H-05, H-06, H-07, H-10, H-11, H-12,
H-13, H-14, H-15, H-16, H-17, H-18, H-19, H-20, H-21, H-22, H-23, H-24,
H-25, H-26, H-31.

**Pre-EN-002 count:** The implementation summary states 31. The TASK-028
effectiveness report confirms "31" in the baseline table. The rules that were
retired are documented: H-08 and H-09 absorbed into H-07, H-27 through H-30
absorbed into H-25 and H-26. That accounts for 6 rules retired (31 - 6 = 25).
The SSOT (quality-enforcement.md) version jumped from 1.3.0 to 1.4.0, consistent
with this change.

**VERDICT: VERIFIED.** Current count is 25. The pre/post delta of -6 is
consistent with the documented consolidations.

**Gap:** The pre-EN-002 state of 31 is taken on faith from the implementation
summary and TASK-028 report. There is no git tag or independent artifact
documenting the baseline. This is an evidentiary gap, not an arithmetic error.

---

### Claim 3: "559 tokens within 850 budget"

**Method:** Re-run the token calculation using the same formula (chars/4 * 0.83,
ceiling) against all 16 markers.

**Computed directly (simulation run):**

| rank | tokens_label | chars | estimated_tokens |
|------|-------------|-------|-----------------|
| 1 | 50 | 161 | 34 |
| 2 | 90 | 165 | 35 |
| 2 | 50 | 264 | 55 |
| 3 | 50 | 92 | 20 |
| 3 | 25 | 62 | 13 |
| 4 | 60 | 220 | 46 |
| 4 | 30 | 95 | 20 |
| 5 | 30 | 69 | 15 |
| 5 | 40 | 102 | 22 |
| 6 | 50 | 275 | 58 |
| 6 | 100 | 286 | 60 |
| 7 | 60 | 140 | 30 |
| 7 | 25 | 116 | 25 |
| 7 | 60 | 195 | 41 |
| 8 | 40 | 118 | 25 |
| 9 | 70 | 317 | 66 |

All 16 items fit within the 850-token budget (cumulative per-item estimation).

The preamble is assembled by joining all 16 marker contents with spaces:
- Total preamble chars: **2692**
- Token estimate of assembled preamble: **559**
- Budget utilization: 559/850 = **65.8%**

**VERDICT: VERIFIED.** The 559 tokens figure is arithmetically correct using
the engine's formula. The 850-token budget is correctly reflected in the code
(`_DEFAULT_TOKEN_BUDGET = 850`).

**Anomaly detected:** The token `tokens` labels in markers are stale relative
to actual computed tokens. For example, rank=2 markers are labeled `tokens=90`
and `tokens=50` respectively, but actual computation yields 35 and 55. The
engine documentation explicitly states it ignores the `tokens` metadata field
and computes its own estimate, so this is not a bug but is a documentation
drift issue. The labels serve no functional purpose yet consume space.

---

### Claim 4: "16 L2 markers across 9 files"

**Method:** Count L2-REINJECT markers in all `.claude/rules/*.md` files.

**File count (by direct inspection):**

Files with at least one L2-REINJECT marker in `.claude/rules/`:
1. architecture-standards.md — 1 marker
2. coding-standards.md — 1 marker
3. mandatory-skill-usage.md — 1 marker
4. markdown-navigation-standards.md — 1 marker
5. mcp-tool-standards.md — 1 marker
6. python-environment.md — 1 marker
7. quality-enforcement.md — 8 markers
8. skill-standards.md — 1 marker
9. testing-standards.md — 1 marker

Files with zero markers: error-handling-standards.md, file-organization.md,
project-workflow.md, tool-configuration.md (4 files, all stubs or delegators).

Total files: **9** with markers. Total markers: **16**.

**VERDICT: VERIFIED.** Claim is exact.

---

### Claim 5: "No L3 AST enforcement broken by consolidation"

**Method:** Search for references to retired H-rule IDs (H-08, H-09, H-27..H-30)
in the L3 enforcement engine source code (`src/infrastructure/internal/enforcement/`).

**Findings:**

The L3 enforcement engine (`pre_tool_enforcement_engine.py`, `enforcement_rules.py`)
does not reference any H-rule IDs by number. It references them by description
(comment `# H-07: Architecture layer isolation`) and by functional behavior
(LAYER_IMPORT_RULES, EXEMPT_FILES). The compound rule H-07 absorbed H-08 and H-09,
but the enforcement behavior in LAYER_IMPORT_RULES is unchanged:

```python
LAYER_IMPORT_RULES: dict[str, set[str]] = {
    "domain": {"application", "infrastructure", "interface"},
    "application": {"infrastructure", "interface"},
    "infrastructure": {"interface"},
    "shared_kernel": {"infrastructure", "interface"},
}
```

The enforcement_rules.py comment was updated from `H-07/H-08` to `H-07 compound`.
The functional enforcement behavior is identical.

References to H-08, H-09, H-27..H-30 exist only in test files (`test_quality_framework_e2e.py`)
where they are explicitly documented as consolidated IDs:

```python
# H-08 and H-09 were consolidated into H-07 per EN-002
consolidated_ids = {"H-08", "H-09"}
```

**VERDICT: VERIFIED.** L3 enforcement is structurally unchanged. The claim is
accurate in scope: consolidation changed the governance ID numbering but did
not alter the enforcement behavior.

**Qualification:** The L3 engine only enforces import boundaries (H-07) and
one-class-per-file (H-10). It does not enforce any of the other 23 HARD rules.
The claim "no L3 enforcement broken" is therefore narrow in scope and trivially
true for all non-H-07/H-10 rules.

---

### Claim 6: "All 3377 tests pass"

**Method:** Run the full test suite.

**Observed output:**

```
3377 passed, 66 skipped in 92.22s (0:01:32)
```

**VERDICT: VERIFIED.** The claim is accurate as of 2026-02-21.

**Qualification:** 66 tests are skipped. The skipped tests are not documented
in the implementation summary. Skips in enforcement-related tests could mask
regressions (e.g., `TestActualFile::test_actual_file_count_within_ceiling` would
skip if `quality-enforcement.md` is not found). The test uses a relative path
(`Path(".context/rules/quality-enforcement.md")`) without CWD assertion, which
could cause silently passing behavior if run from the wrong directory.

---

### Claim 7: "Tier A has 21 rules, Tier B has 4"

**Method:** Count rules in the Two-Tier Enforcement Model table in
quality-enforcement.md and verify each assignment.

**Tier A rules (from the table):**

| Rules | Count |
|-------|-------|
| H-01, H-02, H-03 | 3 |
| H-05, H-06 | 2 |
| H-07, H-10 | 2 |
| H-11, H-12 | 2 |
| H-13, H-14, H-31 | 3 |
| H-15 | 1 |
| H-19 | 1 |
| H-20, H-21 | 2 |
| H-22 | 1 |
| H-23, H-24 | 2 |
| H-25, H-26 | 2 |
| **Total** | **21** |

**Tier B rules:**

| Rule | Count |
|------|-------|
| H-04 | 1 |
| H-16 | 1 |
| H-17 | 1 |
| H-18 | 1 |
| **Total** | **4** |

Combined: 21 + 4 = 25. Matches the total HARD rule count.

**VERDICT: VERIFIED.** The tier classification is arithmetically consistent.

**Concern:** The tier classification is asserted in the document but not
independently enforced. If a new H-rule is added and someone forgets to
classify it, the tier table will silently drift from the HARD Rule Index.
There is no test or CI check that verifies every H-rule in the HARD Rule Index
has a corresponding entry in the Two-Tier Model.

---

## S-012 FMEA

Failure Mode and Effects Analysis. RPN = Severity (1-10) x Likelihood (1-10) x
Detectability (1-10 where 10 = hard to detect).

---

### Component 1: L2 Engine (PromptReinforcementEngine)

| Failure Mode | Effect | Severity | Likelihood | Detection | RPN |
|---|---|---|---|---|---|
| Fail-open swallows read error silently | 16 markers drop to 0; no reinforcement injected per prompt; no observable failure | 8 | 3 | 10 | 240 |
| New rule file added without L2 marker | Coverage regression; new rules have no L2 protection | 6 | 7 | 9 | 378 |
| Marker `tokens` label drifts from actual computation | Labels become misleading; no functional impact since engine ignores them | 2 | 8 | 7 | 112 |
| Budget (850) too tight for future markers | Rules excluded from preamble in rank order; lowest-priority rules silently dropped | 5 | 4 | 6 | 120 |
| Duplicate rank values cause non-deterministic ordering | Rule injection order varies; lower-priority rules may occasionally preempt higher | 4 | 6 | 8 | 192 |
| `.claude/rules/` symlink breaks | Engine falls back to `quality-enforcement.md` only; coverage drops from 84% to 32% | 7 | 2 | 9 | 126 |
| File encoding error on one rule file | That file's markers silently skipped; partial coverage | 4 | 1 | 9 | 36 |

**Highest RPN: "New rule file added without L2 marker" (RPN=378)**

This is the most likely failure mode with the highest undetectability. When a
developer adds a new `.md` file to `.context/rules/` (or `.claude/rules/`), the
engine picks it up automatically via globbing — but only if that file contains
L2-REINJECT markers. If the file has no markers, the engine silently succeeds
while the new rules get no L2 protection. There is no test or CI gate that
enforces "every rule file must have at least one L2 marker."

---

### Component 2: L5 Gate (check_hard_rule_ceiling.py)

| Failure Mode | Effect | Severity | Likelihood | Detection | RPN |
|---|---|---|---|---|---|
| Pre-commit hook only triggers on quality-enforcement.md changes | If H-rule count grows via a different file edit path, gate never fires | 7 | 4 | 8 | 224 |
| Gate is NOT in GitHub Actions CI | Gate bypassed on direct push (no pre-commit), or if pre-commit is not installed | 9 | 5 | 6 | 270 |
| regex brittle — misses H-rules if table format changes | Rule count understated; ceiling breach undetected | 8 | 3 | 8 | 192 |
| Ceiling and count in same file | An adversarial or careless edit can raise the ceiling simultaneously to hide a breach | 6 | 3 | 7 | 126 |
| sys.exit(2) on parse error is fail-closed | Pre-commit fails; developer patches around it to unblock | 4 | 2 | 5 | 40 |
| Test suite uses relative path for actual file test | Test skips if CWD is not project root; regression in CI could silently pass | 6 | 4 | 9 | 216 |

**Highest RPN: "Gate is NOT in GitHub Actions CI" (RPN=270)**

Verified by inspection of `.github/workflows/ci.yml`: the `check_hard_rule_ceiling.py`
script is not listed in any CI workflow step. The L5 gate exists only as a
pre-commit hook. If a developer:
(a) does not have pre-commit installed,
(b) pushes directly with `--no-verify`, or
(c) uses a CI-only merge path that bypasses local hooks,

the ceiling can be silently breached and merged to the main branch.

**Second-highest: "hook trigger limited to quality-enforcement.md" (RPN=224)**

The pre-commit configuration uses `files: \.context/rules/quality-enforcement\.md$`.
This means the gate fires only when `quality-enforcement.md` is in the staged
changeset. A developer could add a new H-rule to a different file (e.g., a new
rule in `skill-standards.md` that references a new H-rule ID, then adds that ID
to `quality-enforcement.md` in a separate subsequent commit) without the gate
firing on the intermediate state. The gate is not commit-atomic across all
governance files.

---

### Component 3: H-Rule Consolidation

| Failure Mode | Effect | Severity | Likelihood | Detection | RPN |
|---|---|---|---|---|---|
| Compound rules harder to cite in violations | Enforcement messages cite "H-07" but sub-rule unclear; reduced actionability | 3 | 6 | 4 | 72 |
| Retired IDs (H-08..H-09, H-27..H-30) cause confusion | Documentation outside this repo still references old IDs; no redirect mechanism | 5 | 7 | 7 | 245 |
| Sub-items (a), (b), (c) not machine-parseable | L5 gate counts H-07 as 1 rule even though it encodes 3 constraints; true rule count understated | 6 | 5 | 8 | 240 |
| Zero headroom blocks EN-001 new rules | EN-001 H-32..H-35 cannot be added; either exception mechanism must be used or further consolidation | 7 | 9 | 2 | 126 |

**Highest RPN: "Retired IDs cause confusion" (RPN=245)**

H-08, H-09, H-27..H-30 are documented as absorbed but are not "tombstoned" in
the SSOT. If an ADR, playbook, or external document references H-08, there is
no forward pointer. The only mitigation is the e2e test comment. Six rules
disappeared from the ID space without an explicit deprecation notice or redirect
table.

---

### Component 4: HARD Rule Ceiling

| Failure Mode | Effect | Severity | Likelihood | Detection | RPN |
|---|---|---|---|---|---|
| Zero headroom at ceiling | Any new rule requires either exception process or consolidation; progress blocked | 7 | 9 | 2 | 126 |
| Exception mechanism itself unconstrained | N<=3 limit and 3-month duration are policy, not enforced; exception could be extended indefinitely | 6 | 4 | 7 | 168 |
| Ceiling derivation based on LLM research claims | "20-25 rules is practical upper bound" — not empirically validated for this specific model | 5 | 6 | 8 | 240 |
| Ceiling set equal to current count | No room for measurement error in counting (any miscounting = automatic breach) | 4 | 3 | 5 | 60 |

**Highest RPN: "Ceiling derivation from unvalidated LLM claims" (RPN=240)**

The ceiling of 25 is justified by three constraint families, the first being
"LLM attention budgets degrade with rule count; 20-25 rules is the practical
upper bound." This is stated as a design rationale, not an empirically derived
value for Claude Sonnet/Opus in the Jerry framework context. The same document
enforces this ceiling with L5 CI. If the actual LLM degradation curve supports
30-35 rules at comparable reliability, the ceiling is needlessly restrictive;
if it supports only 15-20 rules, the ceiling is dangerously permissive.

---

### Component 5: Exception Mechanism

| Failure Mode | Effect | Severity | Likelihood | Detection | RPN |
|---|---|---|---|---|---|
| Exception requires C4 ADR (auto-escalated) creating deadlock | EN-001 H-32..H-35 are already past the 3-slot exception limit (N=4); no compliant path exists | 8 | 9 | 3 | 216 |
| Exception duration not tracked | 3-month reversion deadline not automatically enforced; no CI gate tracks it | 6 | 7 | 8 | 336 |
| Exception ceiling (25+3=28) not updated in L5 gate | If exception is invoked, developer must remember to update the L5 ceiling parameter manually | 7 | 5 | 5 | 175 |

**Highest RPN: "Exception duration not tracked" (RPN=336)**

The exception mechanism defines a 3-month maximum duration and requires a
"consolidation plan." However, there is no automated tracking of exception
expiry. No CI gate, no worktracker reminder, no automated test. The exception
can silently persist beyond the 3-month window with no detection. This is the
highest single RPN across all components.

---

## S-013 Inversion

Each key claim is inverted to expose hidden assumptions and failure modes.

---

### Inversion 1: "What if the ceiling should be HIGHER than 25, not lower?"

**Inverted claim:** The ceiling of 25 HARD rules is too restrictive and should
be raised to 31 or higher.

**Argument for inversion:**

The ceiling was derived partly from "LLM cognitive load" — a claim from training
data, not from empirical testing of this specific model in this context. Claude
Sonnet 4.6 with a 200K context window processes L1 rules (~12,500 tokens) at
session start and L2 markers (~559 tokens) at each prompt. The 25-rule ceiling
was justified by reference to "20-25 rules is the practical upper bound" without
a cited study or ablation.

If 31 rules were functioning adequately before EN-002 (producing correct outputs
in practice), the consolidation down to 25 may have solved a theoretical
problem (the ceiling was "unprincipled") while creating a concrete operational
problem (EN-001 is now blocked). The actual failure mode that motivated EN-002
(DISC-001: unprincipled ceiling, DISC-002: L2 gap) was a documentation and
coverage issue, not a demonstrated enforcement failure at 31 rules.

**Counter-evidence that preserves the original claim:**

The consolidation of H-25..H-30 into 2 rules and H-07..H-09 into 1 rule
preserved all enforcement behavior while reducing cognitive load. The implementation
summary acknowledges "Compound rules harder to reference individually" as a risk.
A ceiling of 25 with compound rules covering more behavior than 31 simple rules
did is a net improvement in cognitive load management, not a reduction.

**Verdict on inversion:** The inversion reveals a real gap: the 25-rule ceiling
is not empirically grounded. A ceiling lower than the previous baseline (31)
may be premature optimization that trades a governance problem for an operational
one. However, the principled derivation from 3 independent constraint families
is stronger evidence than the pre-EN-002 ceiling of 35 (which had no derivation
at all). The ceiling reduction is defensible but should be revisited after
EN-001 encounters the zero-headroom constraint in practice.

---

### Inversion 2: "What if consolidation WEAKENS enforcement?"

**Inverted claim:** Merging H-25..H-30 into H-25/H-26 and H-07..H-09 into
H-07 makes enforcement harder, not easier.

**Argument for inversion:**

Before consolidation, H-27 ("Skill file MUST be exactly `SKILL.md`") was a
distinct, precisely named rule with its own entry. After consolidation, it
became sub-item (a) of H-25 ("Skill naming and structure (SKILL.md case,
kebab-case folder, no README.md)"). The L5 gate now reports 25 rules instead
of 31, but those 25 rules encode more behavioral content per entry.

The L2 marker for H-25/H-26 reads:
```
Skills: SKILL.md exact case, kebab-case folder, no README.md (H-25).
Description WHAT+WHEN+triggers, repo-relative paths, register in
CLAUDE.md+AGENTS.md (H-26). Navigation table REQUIRED (H-23).
```

This marker packs 6 previously distinct rules into 2 marker slots. An LLM
reading "Navigation table REQUIRED (H-23)" in the H-25/H-26 marker context
receives less rule isolation than when H-23 was stated independently in a
dedicated marker (which it still is, separately). The consolidation introduces
rule duplication in the marker space: H-23 is now referenced in TWO different
markers (skill-standards.md rank=7 and markdown-navigation-standards.md rank=7),
creating potential confusion about which marker is authoritative.

More seriously: the sub-items (a), (b), (c) in compound rules have no
machine-parseable structure. The L5 gate counts H-07 as 1 rule even though it
encodes 3 distinct architectural constraints (domain imports, application
imports, composition root). The rule count of 25 understates the actual
behavioral surface area being enforced.

**Counter-evidence that preserves the original claim:**

The consolidation only merged rules with strong thematic cohesion (all
skill-standards rules, all architecture-layer rules). The enforcement behavior
itself was not changed. The L3 engine enforces the same import boundaries
whether H-07 carries 1 or 3 sub-items. The concern about compound rules applies
to the governance documentation layer, not the functional enforcement layer.

**Verdict on inversion:** The inversion reveals a real structural weakness:
compound rules conflate governance count management with behavioral coverage.
The L5 ceiling gate measures rule IDs, not behavioral coverage area. A
"compliant" state at 25 rules could encode more total behavior than the
previous "non-compliant" state at 31 rules. The system should measure behavioral
coverage, not rule count, as its primary effectiveness metric.

---

### Inversion 3: "What if L2 coverage doesn't correlate with enforcement quality?"

**Inverted claim:** The increase from 32% to 84% L2 coverage is a vanity metric
that does not improve actual enforcement quality.

**Argument for inversion:**

L2 coverage measures whether an H-rule is mentioned in a per-prompt marker. It
does not measure:

1. Whether the LLM actually changes its behavior in response to the marker
2. Whether the marker text is specific enough to guide correct behavior
3. Whether the rule is violated less frequently after the change
4. Whether context rot is actually reduced for the newly covered rules

The most critical rules (H-01, H-02, H-03, H-13, H-14) were already in L2
coverage before EN-002. The 11 newly covered rules (H-07, H-10..H-12, H-20..H-26)
are structurally specific (architecture imports, test coverage, skill file naming).
An LLM that would violate H-20 (test before implement) is unlikely to be
corrected by a 22-token marker that says "BDD Red phase: NEVER write
implementation before test fails (H-20)." The constraint is enforced more
reliably by the test suite itself (3377 tests, 90% coverage) and the L3
pre-tool-use engine than by a prompt reminder.

Conversely, rules like H-22 (proactive skill invocation) and H-31 (clarify
before acting) are behavioral and judgment-based. These are exactly where L2
per-prompt reinforcement provides real value — but H-22 and H-31 were already
in L2 coverage before EN-002 (H-31 via rank=2, H-22 newly added via
mandatory-skill-usage.md). The marginal benefit of adding H-07 to L2 when L3
already enforces it deterministically is unclear.

**Counter-evidence that preserves the original claim:**

L2 and L3 enforce against different failure modes. L3 blocks violations
post-hoc (after the tool call is made). L2 prevents violations from being
attempted in the first place, reducing the churn of blocked-then-revised
cycles. Even for rules with L3 enforcement, L2 reminder value exists. The
TASK-028 report quantifies the risk reduction correctly: 21 rules without
L2 coverage dropped to 4, reducing the "context rot exposure" by 81%.

**Verdict on inversion:** The inversion reveals an important measurement gap.
The implementation reports coverage increase as the primary success metric but
does not establish any empirical correlation between L2 coverage and enforcement
outcomes. The TASK-028 report recommendation ("further optimization should be
driven by empirical evidence of enforcement failures") acknowledges this gap but
defers measurement to a future task. The coverage metric is necessary but not
sufficient evidence that enforcement quality improved.

---

### Inversion 4: "What if the L5 gate creates false confidence?"

**Inverted claim:** The L5 gate produces a false sense of security because its
enforcement scope is narrower than its description implies.

**Argument for inversion:**

The implementation summary states "L5 CI enforcement gate preventing silent
ceiling breaches." The description implies the gate prevents ceiling breaches
at the CI level. However:

1. The gate is a **pre-commit hook**, not a CI step. It is not listed in any
   GitHub Actions workflow. A developer who bypasses pre-commit (via
   `--no-verify`, or by not having pre-commit installed) can push a ceiling
   breach to the remote without triggering the gate.

2. The hook's `files:` trigger is `\.context/rules/quality-enforcement\.md$`.
   This means the gate fires only when `quality-enforcement.md` is staged.
   The HARD Rule Index is defined there, so this covers the primary failure
   mode. But it does not cover a scenario where someone mistakenly adds a
   second rule table in another file, or adds H-rule IDs to a file that
   is then cited by quality-enforcement.md.

3. The gate measures count, not behavioral coverage. As noted in the FMEA,
   a developer could "pass" the gate by keeping the count at 25 while
   actually encoding more behavioral surface area via compound sub-items.

4. The statement "L5 enforcement: Active" in the metrics table creates a
   binary impression (active/inactive) that obscures the partial scope. The
   pre-commit hook is one mechanism, but without CI pipeline integration,
   "L5" in the enforcement architecture sense (commit/CI) is only half-active.

**Counter-evidence that preserves the original claim:**

"L5 gate" in the enforcement architecture is defined as "Commit/CI" and
explicitly acknowledges it operates at the git commit boundary. A pre-commit
hook is a valid L5 implementation. The test `TestActualFile::test_actual_file_count_within_ceiling`
in `test_hard_rule_ceiling.py` provides a second detection layer within the
test suite (which does run in CI, via the test matrix). The gate does prevent
the most common silent breach scenario: adding rules to quality-enforcement.md
without checking the ceiling.

**Verdict on inversion:** The inversion reveals a real scope limitation. The
L5 gate is correctly implemented for the scenario it was designed for (careless
addition to quality-enforcement.md), but the claim of "CI enforcement gate" is
overstated for a mechanism that only operates locally as a pre-commit hook. The
`TestActualFile` test is the true CI-level enforcement, and it should be
explicitly documented as such.

---

## Findings

### CRITICAL

None. No finding represents an immediate failure of the implementation's core
objectives. The implementation is functionally correct.

---

### MAJOR

**MAJOR-1: L5 gate is not in CI — "CI enforcement" claim is overstated**

The `check_hard_rule_ceiling.py` script is configured only as a pre-commit hook
triggered on local commits. It is not executed in any GitHub Actions workflow
(verified by inspection of `.github/workflows/ci.yml`). The claim of "L5 CI
enforcement gate" implies CI-level enforcement that does not exist. Developers
without pre-commit installed, or those who use `--no-verify`, can bypass the
gate entirely. The only CI-level enforcement is the `TestActualFile` test within
the unit test suite — which is not explicitly documented as the CI backstop.

**Recommendation:** Add `uv run python scripts/check_hard_rule_ceiling.py` as
an explicit step to the CI workflow, OR explicitly document that
`TestActualFile` is the CI-level enforcement and the pre-commit hook is the
developer-experience layer.

---

**MAJOR-2: Exception mechanism duration is untracked and unenforced**

The exception mechanism defines a 3-month reversion deadline, but there is no
CI gate, automated test, or worktracker integration that enforces this deadline.
An exception can silently persist indefinitely. This is the highest-RPN failure
mode in the FMEA (RPN=336). A governance exception that expires on paper but
persists in the codebase is equivalent to a permanent ceiling increase.

**Recommendation:** Add a test or CI check that reads any active exception ADR
and verifies its expiry date has not passed. Alternatively, encode the expiry
date in the ceiling value comment in quality-enforcement.md so it is visible
during every L5 gate check.

---

**MAJOR-3: EN-001 H-32..H-35 exceeds exception mechanism limits**

The implementation summary notes "EN-001 H-32..H-35 integration requires the
exception mechanism to temporarily expand the ceiling from 25 to 29 (N=4 exceeds
N<=3 maximum)." The exception mechanism's maximum of N=3 additional slots
(ceiling+3=28) is explicitly insufficient for EN-001. EN-001 cannot be
implemented without either: (a) further consolidation to create 4 slots of
headroom, or (b) revising the exception mechanism itself (requiring a C4 ADR
for an already-C3+ change). This is not a defect in EN-002 but is an unresolved
dependency that makes the zero-headroom state a blocking issue, not just an
"active risk."

**Recommendation:** The TASK-028 report's "EN-001 dependency note" should be
elevated to a formal blocker on EN-001 with a concrete resolution path
(consolidation candidates identified, or exception mechanism limit justified
for N=4).

---

### MINOR

**MINOR-1: `tokens` metadata labels in L2 markers are systematically stale**

All 16 L2-REINJECT markers carry a `tokens=N` label. The engine ignores this
field and computes its own estimate. The declared labels diverge significantly
from computed values (e.g., declared 90 vs. computed 35 for the quality gate
marker). This creates maintenance confusion: a developer reading a marker cannot
determine whether 559 tokens was computed from the declared labels or from the
chars-based formula. The labels are non-functional noise.

**Recommendation:** Either (a) remove the `tokens` field from the marker format
specification since it is explicitly ignored, or (b) update all labels to
reflect computed values and add a CI check that verifies label accuracy.

---

**MINOR-2: Two-Tier Model in quality-enforcement.md has no enforcement**

The Two-Tier Model table is a classification artifact: every H-rule is listed
in either Tier A or Tier B. If a new H-rule is added to the HARD Rule Index
without being added to the tier table, the tables silently diverge. No test or
CI gate verifies that the union of (Tier A rules) + (Tier B rules) = (all
H-rules in HARD Rule Index). The only way to detect the drift is manual review.

**Recommendation:** Add a CI test that parses both the HARD Rule Index and the
Two-Tier Model table, verifies their union is equal, and fails if any H-rule
is unclassified.

---

**MINOR-3: Stale test class docstring in TestBudgetEnforcement**

`TestBudgetEnforcement` has the docstring "Tests for 600-token budget constraint
verification" and contains `assert result.token_estimate <= 600` (line 256).
The default budget is now 850. The test passes because the sample content used
by the `engine` fixture produces only ~134 tokens (well under both 600 and 850).
The test passes for the wrong reason: it would still pass even if the budget
was 601, 700, or 850 because the sample data is too small to expose budget
differences. The test does not verify the 850-token budget.

**Recommendation:** Update the test class to assert `<= 850` and add a test
that uses the actual `.claude/rules/` directory to verify the 559-token real
output fits within the 850-token budget.

---

**MINOR-4: Retired H-rule IDs have no tombstone or redirect mechanism**

H-08, H-09, H-27, H-28, H-29, H-30 were retired by consolidation. They are
referenced only in e2e test comments as `consolidated_ids`. Any external
document, playbook, or ADR that references these IDs has no in-SSOT forward
pointer. The quality-enforcement.md HARD Rule Index has a gap between H-07
and H-10, and between H-26 and H-31, with no explanation in the document itself.

**Recommendation:** Add a "Retired Rule IDs" section or footnote to the HARD
Rule Index that lists absorbed IDs and their destination rule IDs. This costs
no budget (it is prose, not table rows) but provides clear traceability.

---

### OBSERVATION

**OBS-1: L2 coverage metric is necessary but not sufficient**

The 32% to 84% improvement is a structurally valid measurement. However, no
empirical evidence links L2 coverage percentage to enforcement failure rates.
The TASK-028 report acknowledges this by deferring further optimization to
"empirical evidence of enforcement failures." This is the correct approach but
implies the 84% metric should be treated as a leading indicator, not a lagging
one. Future EN-002 follow-up work should establish a baseline failure rate
for Tier B rules (H-04, H-16, H-17, H-18) and measure whether it is acceptable.

---

**OBS-2: Ceiling derivation relies on non-empirical LLM cognitive load claim**

The ceiling of 25 is partly justified by "LLM attention budgets degrade with
rule count; 20-25 rules is the practical upper bound." This claim is drawn from
general ML research and training data, not from empirical testing of Claude
Sonnet/Opus in this specific context. The ceiling is not fragile (it has two
other derivation families), but the cognitive load family is the weakest leg.
If the ceiling needs to be revised in the future, the cognitive load justification
should be empirically tested rather than assumed.

---

**OBS-3: The L2 engine is the only system-level component that expands to
cover new rule files automatically**

The engine's `sorted(rules_path.glob("*.md"))` pattern is genuinely
forward-compatible. Every other component (L5 gate, Tier Model, HARD Rule Index,
L3 engine) requires explicit manual update when rules change. The L2 engine is
uniquely self-updating for new rule files. This is an architectural strength
that should be acknowledged and preserved in future engine changes.

---

**OBS-4: Boundary between "L5 gate" and "test suite" blurs enforcement layer definitions**

The ADR-EPIC002-002 enforcement architecture defines L5 as "Commit/CI." The
implementation uses both a pre-commit hook (commit-level) and a test
(`TestActualFile`, CI-level). These are structurally different detection layers
but are both described under "L5." The architecture spec should either
sub-classify L5 into L5a (commit hook) and L5b (CI), or explicitly acknowledge
that a single test provides the CI half of L5 for the ceiling check.
