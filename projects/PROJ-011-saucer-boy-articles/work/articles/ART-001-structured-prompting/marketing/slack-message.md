# Slack Message — Blog Post Promotion

## Short Version

Your LLM output looks authoritative. Clean headings, professional language, confident tone.

Except it's a mirage — at least without structure. I call it the fluency-competence gap: models learn to *sound* like they understand without actually understanding. And RLHF can amplify sycophantic tendencies — models learn to agree with you rather than push back, even when you're wrong.

Three levels of prompting address this. Level 1 gets you polished garbage. Level 2 gets you something you can actually use. Level 3 gets you work that survives scrutiny.

The difference isn't the model. It's what you told it about what good looks like.

Full breakdown with research citations: [BLOG_URL]

## Longer Version

Shane McConkey showed up to ski competitions in a banana suit and still won. The guy looked completely unhinged on the mountain. He wasn't. Every wild thing he did was backed by obsessive preparation.

Same dynamic applies to LLM prompting. Three levels:

**Level 1:** "Apply frameworks to my repo." You get something that *looks* like an expert wrote it. The expert part is a mirage.

**Level 2:** "Research 10 frameworks, cite sources, show selection criteria, comparison table." Two sentences more specific. Most of the benefit lives here.

**Level 3:** Parallel work streams. Self-critique against dimensions you define. Human checkpoints. Plan before product. For when getting phase one wrong means everything after it looks authoritative but is structurally broken.

The move most people miss: plan in one conversation, execute in a fresh one. Context windows are finite — at least one study on document retrieval found models attend more to the beginning and end of long contexts than the middle. Your carefully crafted instructions from message three are competing with forty messages of planning debate, and the model isn't weighing them evenly.

Works on Claude, GPT, Gemini, Llama — every model I've tested across ~200 sessions. Structure in, structure out.

Full post with the research behind it: [BLOG_URL]
